# 模型单元（开源模型场景）
本文为您介绍如何上传开源模型，并使用模型单元进行部署。
> 模型单元是方舟提供的一种在线推理部署方式，支持模型独占算力部署，能够为用户提供稳定且高效的推理服务，适用于对性能和稳定性要求较高的业务场景，更多介绍请参见 [模型单元](https://www.volcengine.com/docs/82379/1568332)。

# 主要优势
使用模型单元部署开源模型，在业务流量相对大的场景下有明显优势，例如支持多机部署，支持灵活的弹性能力，夜间需要的资源更少等。对流量较小的场景，由于需要占满起步资源量，优势不明显。
# 支持范围
## 支持模型
当前支持上传以下基础模型，及基于以下基础模型的精调后模型；如果是LoRA精调模型，需上传合并LoRA权重的模型文件。

* [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B)
* [Qwen2.5-3B](https://huggingface.co/Qwen/Qwen2.5-3B)
* [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B)
* [Qwen3-8B](https://huggingface.co/Qwen/Qwen3-8B)
* [Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)

## 文件格式要求
应为 hugging face 模型文件格式。
## 量化方式要求
目前量化方式支持 W8A8、W4A8 等（每个模型支持的量化方式可能不同）。同时支持上传未量化的模型，您可通过控制台查看或联系技术支持人员确认。
# 计费说明
模型单元按照您选择的机型和使用时长进行收费，不支持按Token 计费。
[详细计费说明](https://www.volcengine.com/docs/82379/1568332#%E8%AE%A1%E8%B4%B9%E8%AF%B4%E6%98%8E)
[计费价格](https://www.volcengine.com/docs/82379/1544106#c26435c9)
# 使用流程
## 步骤一：准备工作
> 本功能为邀测功能，下述流程有任何疑问，可联系您的销售或技术支持。 

1. 联系您的销售人员，说明您要使用的开源模型和版本，并 [申请共享模型权限](https://applink.larkoffice.com/T8QpXfzOYS4k)。 方舟会在项目交付阶段将所需模型卡片共享出来，您仅需从【[模型仓库](https://console.volcengine.com/ark/region:ark+cn-beijing/customModel?Original=%7B%7D&tab=Original)】或【[资源共享](https://console.volcengine.com/resourcemanager/region:resource-manager+cn-beijing/resource_share/shared)】页面接受共享，如下图所示：
   说明：此步骤的作用是系统匹配模型单元模板（包括支持的机型和部署方式）。成功获取模型卡片后，无需做任何操作。
   <div style="text-align: center"><img src="https://p9-arcosite.byteimg.com/tos-cn-i-goo7wpa0wc/f0884ec6bfee4574bb59d981a1f5e71b~tplv-goo7wpa0wc-image.image" width="1916px" />   </div>

2. 联系您的销售人员，[申请使用模型单元功能](https://applink.larkoffice.com/T8WyQLB0QzAZ)。申请成功后，您会获得一个机型数量的初始值。如果需要扩充资源，可再次联系销售人员，说明扩充需求，经审批通过后即可增加相应的机型数量。 

## 步骤二：上传模型

1. 将您的开源模型文件上传至火山引擎对象存储（TOS），操作步骤可参见 [TOS 快速入门](https://www.volcengine.com/docs/6349/74830)。
2. 单击 [模型上传](  https://console.volcengine.com/ark/region:ark+cn-beijing/customModel/create)，将您存储在 TOS 中的开源模型文件上传至火山方舟模型仓库。
   * 基础模型：选择您的开源模型所属的基础模型。
   * 模型目录：选择您上一步存放的 TOS 目录。

![Image](https://p9-arcosite.byteimg.com/tos-cn-i-goo7wpa0wc/4b213e93e22f40c0b651dfc1e26931d1~tplv-goo7wpa0wc-image.image =1702x)

## 步骤三：部署模型单元

1.  单击 [创建推理接入点](https://console.volcengine.com/ark/region:ark+cn-beijing/endpoint/create)，使用模型单元方式接入您的开源模型。
   * 接入模型：选择火山方舟平台 > 添加模型 > 模型仓库 > 选择您步骤二上传的开源模型。
   * 接入模式：选择模型单元。
2. 其他配置根据页面指引进行选择。
3. 勾选协议，并单击 **创建并接入**，完成下单。

# 调用方式
模型单元部署成功后，您可通过 [Chat API](https://www.volcengine.com/docs/82379/1494384) 调用推理服务。注意 model 需使用  Endpoint ID。 

# 常见问题
**Q1：使用方舟部署模型单元，与使用 MLP/IaaS 上自建相比有什么优势？**
A1：方舟提供持续优化的推理引擎、弹性扩缩容和 Cache 等能力，在相同资源下延迟和成本更优。弹性是指资源随请求压力自动扩缩，运维和配置难度更低。
**Q2：是否可以自由上传任意模型文件？**
A2：控制台上传功能仅支持上传基础模型和量化方式为平台明确支持的模型，文件格式应为 hugging face 模型文件格式；其他开源模型和量化方式，必须经过产研评估后适配部署。
**Q3：什么是量化？我使用的模型量化方式不在当前支持范围怎么办？上传的模型文件是否能在平台上做量化？**
A3：量化是将模型参数精度降低以减少计算和存储消耗。如用户使用了特定的量化算法请提前沟通。
由于上传的自定义模型在模型结构上可能存在差异，平台短期内暂不支持通用的量化产品化方案。您需自行验证量化后模型的性能与效果，注意量化前后可能出现的效果差异；方舟团队支持在必要时提供参考脚本或建议。