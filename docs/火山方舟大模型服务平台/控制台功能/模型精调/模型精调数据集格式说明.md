# 模型精调数据集格式说明
当进行模型精调前需要准备训练和验证的数据集。本文详细规范了模型训练的数据集格式要求提供JSONL文件结构、字段说明、示例代码及辅助工具，帮助你准备符合规范的训练数据。
请参考下面的具体格式示例，每个示例后提供了样例文件。
# SFT精调
## 文本生成模型
### 格式示例
样本格式：为JSONL文件（ **JSON Lines**，是一种轻量级的文本文件格式，核心规则 **每一行对应一个独立的、合法的 JSON 对象**），需确保**单个对话样本独占一行**，示例如下。您也可以[下载样例文件](https://ark-cdn.tos-cn-beijing.volces.com/samples/SFT_Text_Sample.jsonl)阅读。
```JSON
{"messages":[{"role":"system","content":"请根据古诗内容，仅回复作者的名字。"},{"role":"user","content":"姑苏城外寒山寺，夜半钟声到客船"},{"role":"assistant","reasoning_content":"用户让我根据古诗内容回复作者名字，诗句是“姑苏城外寒山寺，夜半钟声到客船”。首先，我需要确定这句诗出自哪首诗。这句诗很有名，应该是张继的《枫桥夜泊》。记得以前学过，这首诗的作者是唐代诗人张继。用户要求仅回复作者名字，所以不需要其他内容。检查一下搜索到的信息，虽然这里显示没有具体资源，但根据已有的知识，确定作者是张继没错。所以直接回复名字即可，不用多余的解释。","content":"张继","loss_weight":1}]}
{"messages":[{"role": "user", "content": "什么时候需要SFT？"}, {"role": "assistant", "content": "通过 prompt engineering 无法解决或 prompt 中描述过于复杂时；对大模型输出内容有格式要求时，而模型仍有部分 case 不符合要求；期望通过 SFT 来减少 prompt 中的内容，加速线上推理的耗时。"}]}
```

### 格式说明
为便于展示各个字段关系，将 JSONL 格式文件的一条数据展开，如下：
```JSON
{
    "messages": [
        {
            "role": "system",
            "content": "请根据古诗内容，仅回复作者的名字。"
        },
        {
            "role": "user",
            "content": "姑苏城外寒山寺，夜半钟声到客船"
        },
        {
            "role": "assistant",
            "reasoning_content": "用户让我根据古诗内容回复作者名字，诗句是 “姑苏城外寒山寺，夜半钟声到客船”。首先，我需要确定这句诗出自哪首诗。这句诗很有名，应该是张继的《枫桥夜泊》。记得以前学过，这首诗的作者是唐代诗人张继。用户要求仅回复作者名字，所以不需要其他内容。检查一下搜索到的信息，虽然这里显示没有具体资源，但根据已有的知识，确定作者是张继没错。所以直接回复名字即可，不用多余的解释。",
            "content": "张继",
            "loss_weight": 1
        }
    ]
}
```

字段说明：
每行一条JSON格式的数据：

* **messages** *(list, required)*: 描述一个对话列表
   * **role** *(str, required)*: 角色，system、user、assistant中的一个
      :::tip
      **特别说明**：如希望提升模型的Function Calling能力，需选择支持Function Calling训练的模型并提供包含Function Calling格式数据的训练集用于精调，具体模型和格式可参考[精调优化](/docs/82379/1262342#f118e5a2)。
:::
   * **content** *(str, required)*: 对话内容文本
   * **loss_weight** *(float, optional)*: 对于内容的loss训练权重。当role=system/user，loss_weight默认值为0.0且不可修改；当role=assistant，loss_weight默认值为1.0。通过loss_weight字段，可以在训练数据中修改默认值，取值范围是[0.0, 1.0]
   * **reasoning_content** *(str, optional)*: 模型深度思考内容，只允许最后一个 role 为 assistant 的 message 携带。
      * 由于豆包模型在多轮对话的推理过程中，之前轮次对话的reasoning_content不会参与推理。所以为了保证精调后推理效果，请仅在最后一轮对话中携带reasoning_content。
         * 如有优化多轮对话场景下深度思考能力的精调需求，请将一条携带多轮reasoning_content的样本，拆分为多条仅最后一轮对话携带reasoning_content的样本。更多内容请参考[附2：多轮reasoning_content的样本文件拆分](/docs/82379/1099461#5d730edd)。
         * 如多轮对话场景训练时仅最后一轮对话携带reasoning_content，模型将学习到在中间对话轮次不进行thinking，影响模型thinking效果。
         ```Plain Text
         举个例子，假设用户样本是 up1 assistant1（with cot）up2 assistant2（with cot）up3 assistant3（with cot），应该分别拆成以下三条样本进行训练：
         
         up1 assistant1（with cot）
         up1 assistant1（without cot & loss_mask = 0）up2 assistant2（with cot)
         up1 assistant1（without cot & loss_mask = 0）up2 assistant2（without cot & loss_mask = 0）up3 assistant3（with cot）
         ```

   * **thinking** (str, 支持该参数的模型可选填，默认值为disabled、不支持的禁填)：部分具备深度思考能力的模型支持通过 **thinking** 字段控制是否启用深度思考能力。

> 注意：推理和精调模式需要对齐，精调后推理的配置默认是 thinking:enable，如果训练时未设置 thinking参数，则默认值为disabled，需要用户在推理时显示设置 thinking=disabled

      ```JSON
      {
        "messages": [
          {
            "role": "user",
            "content": "什么是SFT？"
          },
          {
            "role": "assistant",
            "content": "在自然语言处理（NLP）领域，Supervised Finetuning（SFT）是一种至关重要的技术手段，用来提升大模型在某一特定领域的表现。通过精细的策划和实施，SFT能够指导模型的学习过程，确保其学习成果与既定目标高度吻合。"
          }
        ],
        "thinking": "disabled"
      }
      ```

      * 不同模型可配置的枚举值不同，详细信息请参见[开启/关闭深度思考](/docs/82379/1449737#fa3f44fa)。
         
- 模型版本 | enabled | disabled | auto
- doubao-seed-1-6-250615 | ✅ | ✅ | ✅
- doubao-seed-1-6-flash-250615 | ✅ | ✅ | ❌
- doubao-1-5-vision-pro-250328 | 不支持该参数，不可携带
- doubao-1-5-pro-32k-250115 | 不支持该参数，不可携带
- doubao-1-5-lite-32k-250115 | 不支持该参数，不可携带

         * `enabled` 表示强制模型进行思考，此时该样本需要携带 reasoning_content 字段，否则将影响模型效果
         * `disabled` 表示强制模型不进行思考，此时该样本不能携带 reasoning_content 字段，否则将影响模型效果
         * `auto` 表示允许模型根据问题自主判断是否需要思考，复杂问题进行思考，简单问题直接回答。此模式下用户自主决定是否希望模型思考。如果希望模型思考，则携带 reasoning_content 字段，否则则不携带。
         * 对于上述格式要求，存量数据集可参考[附1：数据集Thinking字段处理工具](/docs/82379/1099461#089b8570)进行格式转换。

## 文本向量化模型
### **格式示例**
样本格式：为JSONL文件（ **JSON Lines**，是一种轻量级的文本文件格式，核心规则 **每一行对应一个独立的、合法的 JSON 对象**），需确保**单个对话样本独占一行**，示例如下。您也可以[下载样例文件](https://ark-cdn.tos-cn-beijing.volces.com/samples/SFT_TextEmbedding_Sample.jsonl)阅读。
```Plain Text
{"query":"乐清市珍俊服装店","docs":[{"text":"标题：先跪着把钱挣了.才能站着做选择.一定要好好赚钱.人这一生.","label":0,"loss_weight":0},{"text":"标题：看来彩礼是要高了些..CCC#服装人#抖音助手#做个快乐的女孩#实体店#穿搭#实体王府大酒店","label":1,"loss_weight":0},{"text": "标题：最近很喜欢一段话：照顾好自己的健康和情绪，这场人生，你就赢了一大半，其余的其余，人生自有","label":0,"loss_weight":0}]} 
```

### **格式说明**
为便于展示各个字段关系，将JSONL格式文件的一条数据展开，如下：
```JSON
{
    "query": "乐清市珍俊服装店",
    "docs": [
        {
            "text": "标题：先跪着把钱挣了.才能站着做选择.一定要好好赚钱.人这一生.",
            "label": 0
        },
        {
            "text": "标题：看来彩礼是要高了些..CCC#服装人#抖音助手#做个快乐的女孩#实体店#穿搭#实体王府大酒店",
            "label": 1
        },
        {
            "text": "标题：最近很喜欢一段话：照顾好自己的健康和情绪，这场人生，你就赢了一大半，其余的其余，人生自有",
            "label": 0
        }
    ]
}
```

每行一条JSON格式的数据：

* **query***(str, required)*：检索问题文本
* **docs** *(list, required)*：与检索问题文本对应的正负例目标文本列表
   * **text***(str, required)*：正负例文本内容
   * **label***(bool, required)*：label值为1表示该样本为正例，一条数据中需要有1个正例；label值为0表示该样本为负例，一条数据中可包含0-5个负例

## 视觉理解模型
视觉理解模型数据集格式整体与文本生成模型数据格式对齐，额外支持在对话内容中当 “role” 为 “user” 时，于 “content” 字段内通过数组格式混排多模态内容。 
### 图片格式要求

* **数据集文件大小**：单数据集文件大小需控制在 2G 以下。
* **图片规格**：单张图片大小限制在 10M 以内，长宽比要小于 200。单图最大支持 5120 tokens，若超出此数值则会被采样到最大值（图片 token = 长像素 * 宽像素 / 784）。
* **样本长度**：单样本长度若超过模型训练窗口会被截断，图片按单张整体截断。不同模型的窗口大小如下： 
   * doubao-seed-1-6-250615：128k
   * doubao-seed-1-6-flash-250615：128k
   * doubao-1-5-vision-pro-250328：64k
   * doubao-1-5-vision-pro-250115：32k
   * doubao-1-5-vision-lite-250315：128k 
* **支持的图片格式**：参见下表。
   
- **图片格式** | **文件扩展名** | 内容格式 Content Type
- JPEG | .jpg, .jpeg | image/jpeg
- PNG | .apng, .png | image/png
- GIF | .gif | image/gif
- WEBP | .webp | image/webp
- BMP | .bmp | image/bmp
- TIFF | .tiff, .tif | image/tiff
- ICO | .ico | image/x-icon
- DIB | .dib | image/bmp
- ICNS | .icns | image/icns
- SGI | .sgi | image/sgi
- JPEG2000 | .j2c, .j2k, .jp2, .jpc, .jpf, .jpx | image/jp2

### 图片传入方式
在视觉理解模型的数据输入中，图片的传入方式有多种，以满足不同场景的需求，以下是具体介绍。
#### **1.（推荐）使用 TOS 存储图片**

1. 创建存储桶（Bucket）用于存储文件（Object），创建存储桶详细步骤可见[对象存储控制台快速入门](https://www.volcengine.com/docs/6349/74830)。例如创建如下结构：

```Bash
bucketname    #桶名称
├── sample  #桶内文件夹名称
    ├── data.jsonl
└── image   #桶内文件夹名称
    ├── xxx.png
    └── xxxx.png
```

2. jsonl文件中在URL内以“`tos://${bucket}/${object}`”的形式填入TOS对象地址。**注意，TOS URL所引用的图片需与JSONL文件处于同一个桶内。**这种格式能够加快数据处理速度、减少多角色传输所耗费的时间，进而提高训练效率。以上步骤完成后，**精调任务进行数据配置时选择 从TOS中导入方式 仅上传jsonl文件即可**。

```JSON
{
  "messages": [
    {
      "role": "user",
      "content": [
         {
          "type": "image_url",
          "image_url": {
            "url": "tos://bucketname/image/xxx.png"
          }
        },
        {
          "type": "text",
          "text": "What’s in this image?"
        }        
      ]
    },
    {
      "role": "assistant",
      "content": "It is the logo of Doubao LLM model."
    }
  ]
}
```

#### **2.  图片使用**base64编码
##### **格式示例**
将图片进行 base64 编码后填入 jsonl 文件中，需确保**单个样本独占一行**，示例如下。您也可以[下载样例文件](https://ark-project.tos-cn-beijing.volces.com/jsonl/vlm_data.jsonl)参考。
```Plain Text
{"messages":[{"role":"user","content":[{"type":"text","text":"What’s in this image?"},{"type":"image_url","image_url":{"url":"data:image/jpeg;base64,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"}}]},{"role":"assistant","content":"It is the logo of Doubao LLM model."}]}
{"messages":[{"role":"user","content":[{"type":"text","text":"What’s in this image?"},{"type":"image_url","image_url":{"url":"data:image/jpeg;base64,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"}}]},{"role":"assistant","content":"It is the logo of Doubao APP."}]}
```

**格式说明**
为便于展示各个字段关系，将JSONL格式文件的一条数据展开，如下：
```JSON
{
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "image_url",
          "image_url": {
            "url": "data:image/{image_extension};base64,{image_base64}"
          }
        },
        {
          "type": "text",
          "text": "What’s in this image?"
        }
      ]
    },
    {
      "role": "assistant",
      "content": "It is a cat."
    }
  ]
}
```

* 支持在 user role 内混排图文，图文顺序无限制，训练按样本中的顺序进行。
* 支持多轮对话和单轮对话多图。
* text 字段不能为空字符串。
* 支持在创建精调任务时上传或选择多个文件或数据集，**单数据文件大小限制在 2G 以下，大文件可拆分为多个小文件。**

##### **附：图片编码方法**
以下是Python对图片进行 base64 编码的代码示例。
```Python
import base64

# 将结果填入 url 字段内
def encode_image(image_path):
    extension = image_path.split(".")[-1]
    with open(image_path, "rb") as image_file:
        base64_image = base64.b64encode(image_file.read()).decode('utf-8')
        return f"data:image/{extension};base64,{base64_image}"
```

#### 3. 图片使用相对路径
##### **格式示例**
以下为 jsonl 文件中使用图片相对路径的格式示例。

1. 创建目录结构如下：

```Bash
vlm
├── data.jsonl
└── image
    ├── doubao_app_logo.jpeg
    └── doubao_logo.jpeg
```

2. 则对应的数据示例如下：JSONL 文件中需确保**单个样本独占一行**

```Plain Text
{"messages": [{"role": "user","content": [{"type": "image_url","image_url": {"url": "file:./image/doubao_logo.jpeg"}},{"type": "text","text": "What’s in this image?"}]},{"role": "assistant","content": "It is the logo of Doubao LLM model."}]}
```

##### 格式说明
为便于展示各个字段关系，将JSONL格式文件的一条数据展开，如下：
```JSON
{
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "file:./image/doubao_logo.jpeg"
                    }
                },
                {
                    "type": "text",
                    "text": "What’s in this image?"
                }
            ]
        },
        {
            "role": "assistant",
            "content": "It is the logo of Doubao LLM model."
        }
    ]
}
```

**注意：这种格式仅支持文件夹内图片数量少于 1000 张，大于该规模仍建议使用 base64 格式。**

* 具体操作：创建文件夹，将图片和对应的 jsonl 文件按照上述格式要求进行整理后放入该文件夹中。 如在控制台页面进行精调数据配置**需将创建的文件夹拖拽到上传框内**，暂不支持点击上传框后选择文件夹。例如：上方目录结构需把`vlm`文件夹拖拽上传。对于 tos 数据集文件夹路径，可以在控制台直接点击选择。
* 支持图片和 jsonl 文件分别存放在同一文件夹中，通过相对路径在 jsonl 文件中指向图片（暂不支持绝对路径 tos url），文件夹内可包含多个 jsonl 文件，但要求 jsonl 文件位于文件夹根路径。

### **视频抽帧图片序列**
视觉理解模型精调支持用户将视频抽帧为图片后，将时间戳和图片按顺序编排到训练集中。可参考[附3：对视频内容进行抽帧](/docs/82379/1099461#5af16679)。
**格式示例**
样本格式：为JSONL文件（ **JSON Lines**，是一种轻量级的文本文件格式，核心规则 **每一行对应一个独立的、合法的 JSON 对象**），需确保**单个样本独占一行，**示例如下：
```Plain Text
{"messages": [{"role": "user","content": [{"type": "text","text": "你觉得这个恐怖吗？"},{"type": "text","text": "[0.0 second]"},{"type": "image_url","image_url": {"url": "base64/相对路径"}},{"type": "text","text": "[1.0 second]"},{"type": "image_url","image_url": {"url": "base64/相对路径"}},{"type": "text","text": "[2.0 second]"},{"type": "image_url","image_url": {"url": "base64/相对路径"}}]}]}
```

为便于展示各个字段关系，将JSONL格式文件的一条数据展开，如下：
```JSON
{
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "你觉得这个恐怖吗？"
        },
        {
          "type": "text",
          "text": "[0.0 second]"
        },
        {
          "type": "image_url",
          "image_url": {
            "url": "base64/相对路径"
          }
        },
        {
          "type": "text",
          "text": "[1.0 second]"
        },
        {
          "type": "image_url",
          "image_url": {
            "url": "base64/相对路径"
          }
        },
        {
          "type": "text",
          "text": "[2.0 second]"
        },
        {
          "type": "image_url",
          "image_url": {
            "url": "base64/相对路径"
          }
        }
      ]
    }
  ]
}
```

## 视频生成模型
**格式示例：**
样本格式：为JSONL文件（ **JSON Lines**，是一种轻量级的文本文件格式，核心规则 **每一行对应一个独立的、合法的 JSON 对象**），需确保**单个对话样本独占一行**，示例如下。
```Bash
{"output": {"content": {"video_url": "tos://${bucket}/${object}"}}}
```

**格式说明：**
为便于展示各个字段关系，将 JSONL 格式文件的一条数据展开，如下：
```Bash
{
  "output": {
    "content": {
      "video_url": "tos://${bucket}/${object}"
    }
  }
}
```

字段说明：
每行一条JSON格式的样本数据：

* **output** *(list, required)*：一条样本中的输出结构
   * **content** *(list, required)*: 输出内容列表
      * **video_url** *(str, required)*: 视频文件TOS url，形式为："tos://${bucket}/${object} "

视频文件需要先上传到TOS存储中，然后在JSONL文件中的 video_url 内填入 tos 对象地址，**请注意**视频对象需要与JSONL文件在同一个桶内。
**具体操作：**

1. 创建存储桶（Bucket）用于存储文件（Object），创建存储桶详细步骤可见[对象存储控制台快速入门](https://www.volcengine.com/docs/6349/74830)。例如创建如下结构：

```Bash
bucketname    #桶名称
├── sample  #桶内文件夹名称
    ├── data.jsonl
└── video   #桶内文件夹名称
    ├── xxx.mp4
    └── xxxx.mp4
```

2. jsonl文件中在URL内以“`tos://${bucket}/${object}`”的形式填入TOS对象地址，例如上方结构URL则为“`tos://bucketname/video/xxx.mp4`”。**注意，TOS URL所引用的图片需与JSONL文件处于同一个桶内。**这种格式能够加快数据处理速度、减少多角色传输所耗费的时间，进而提高训练效率。以上步骤完成后，**精调任务进行数据配置时选择 从TOS中导入方式 仅上传jsonl文件即可**。

**视频文件限制**

* 分辨率最短边480p及以上（宽和高都大于等于480为判断标准）
* 最大宽高比 < 21:9
* 时长 1-12 秒
* 帧率大于等于 24 fps

# 直接偏好学习（DPO）
## 文本生成模型
### 标准格式
#### **格式示例**
样本格式：为JSONL文件（ **JSON Lines**，是一种轻量级的文本文件格式，核心规则 **每一行对应一个独立的、合法的 JSON 对象**），需确保**单个对话样本独占一行**，示例如下。支持两个回答正负例的偏好对比学习，您也可以[下载样例文件](https://ark-cdn.tos-cn-beijing.volces.com/samples/DPO_Text_Sample.jsonl)阅读。
```Plain Text
{"messages":[{"role":"system","content":"This is a system"},{"role":"user","content":"What your name?"},{"role":"assistant","content":"My name is doubao."},{"role":"user","content":"How to learn Python?"},{"role":"assistant","chosen":"It's so easy. First, you need to learn Python syntax...","rejected":"Check python doc yourself"}]}
```

#### 格式说明
为便于展示各个字段关系，将JSONL格式文件的一条数据展开，如下：
```JSON
{
    "messages": [
        {
            "role": "system",
            "content": "This is a system"
        },
        {
            "role": "user",
            "content": "What is your name?"
        },
        {
            "role": "assistant",
            "content": "My name is doubao."
        },
        {
            "role": "user",
            "content": "How to learn Python?"
        },
        {
            "role": "assistant",
            "chosen": "It's so easy. First, you need to learn Python syntax...",
            "rejected": "Check python doc yourself"
        }
    ]
}
```

每行一条JSON格式的数据：

* **messages** *(list, required)*: 描述一个对话列表
   * **role** *(str, required)*: 角色，取值为system、user、assistant中的一个。最后一个message的role必须是assistant
   * **content** *(str, required)*: 对话内容文本，最后一个message不包含
   * **chosen** *(str, required)***：**偏好的内容正例，最后一个message必须包含
   * **rejected** *(str, required)*：不偏好的内容负例，最后一个message必须包含

### **高级格式**
高级格式支持提供多个模型回复列表，通过 score 来指定对回复的偏好。训练过程将根据 score 的大小自动生成两两对比的偏序对进行训练，注意：score 相同的两个回复不会生成相应的偏序对。
#### **格式示例**
需确保**单个对话样本独占一行**，示例如下。您也可以[下载样例文件](https://ark-project.tos-cn-beijing.volces.com/jsonl/dpo.yaml)阅读。
```Plain Text
{"messages":[{"role":"system","content":[{"text":"This is a system"}]},{"role":"user","content":[{"text":"What your name?"}]},{"role":"assistant","content":[{"text":"My name is doubao."}]},{"role":"user","content":[{"text":"How to learn Python?"}]},{"role":"assistant","content":[{"text":"I don't know!","score":0.5},{"text":"Check python doc yourself","score":0.1},{"text":"It's so easy. First, you need to learn Python syntax...","score":1,"lm_loss_mask":1}],"loss_weight":1}]}
```

#### 格式说明
为便于展示各个字段关系，将文件的一条数据展开，如下：
```JSON
{
    "messages": [
        {
            "role": "system",
            "content": [
                {
                    "text": "This is a system"
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "text": "What is your name?"
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "My name is doubao."
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "text": "How to learn Python?"
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "I don't know!",
                    "score": 0.5
                },
                {
                    "text": "Check python doc yourself",
                    "score": 0.1
                },
                {
                    "text": "It's so easy. First, you need to learn Python syntax...",
                    "score": 1,
                    "lm_loss_mask": 1
                }
            ],
            "loss_weight": 1
        }
    ]
}
```

每行一条JSON格式的数据：

* **messages** *(list, required)*: 描述一个对话列表
   * **role** *(str, required)*: 角色，system、user、assistant中的一个。最后一个message的role必须是assistant，且仅最后一个 message 可以携带多个回复的偏序对
   * **content** *(list, required)*: 对话内容文本或者一个list对象。**最后一个message的content必须为list类型，且list长度在2～5之间**。list的元素为dict，包含以下字段：
      * **text** *(str, required)*：文本内容
      * **score** *(float, required)*：偏好值，范围在0～1的浮点数，越高表示越偏好
      * **lm_loss_mask** *(float, required)*：该条消息是否计算 sft loss，默认为 0，即不计算 sft loss
   * **loss_weight** *(float, optional)*: 该条 message loss 的加权系数，默认为 1

## 视觉理解模型
支持提供多个模型回复列表，通过 score 来指定对回复的偏好。训练过程将根据 score 的大小自动生成两两对比的偏序对进行训练，注意：score 相同的两个回复不会生成相应的偏序对。
支持在role为user的对话的content字段中通过数组混排图文。图片格式请参考 [图片格式要求](/docs/82379/1099461#65245bbb)，图片传入方式请参考[图片传入方式](/docs/82379/1099461#6609c8a3)。
### 格式示例
样本格式：为JSONL文件（ **JSON Lines**，是一种轻量级的文本文件格式，核心规则 **每一行对应一个独立的、合法的 JSON 对象**），需确保**单个对话样本独占一行**，示例如下。
```Plain Text
{"messages":[{"role":"system","content":[{"type":"text","text":"This is a system"},{"type":"image_url","image_url":{"url":"tos://bucket/1.jpg"}}]},{"role":"user","content":[{"type":"text","text":"What's your name?"},{"type":"image_url","image_url":{"url":"tos://bucket/1.jpg"}}]},{"role":"assistant","content":[{"type":"text","text":"My name is doubao."}],"loss_weight":1.0},{"role":"user","content":[{"type":"text","text":"How to learn Python?"}]},{"role":"assistant","choices":[{"content":[{"type":"text","text":"I don't know!"}],"score":0.5,"reasoning_content":"xxx","lm_loss_mask":0},{"content":[{"type":"text","text":"Check python doc yourself"}],"score":0.1,"lm_loss_mask":0},{"content":[{"type":"text","text":"It's so easy. First, you need to learn Python syntax..."}],"score":1.0,"lm_loss_mask":0},{"content":[{"type":"text","text":"It's so easy. First, you need to learn Python syntax..."}],"score":1.0,"lm_loss_mask":0}]}],"thinking":"disabled"}
```

### 格式说明
为便于展示各个字段关系，将JSON格式文件的一条数据展开，如下：
```Python
{
  "messages": [
    {
      "role": "system",
      "content": [
        {"type": "text", "text": "This is a system"},
        {"type": "image_url", "image_url": {"url": "tos://bucket/1.jpg"}}  
      ]
    },
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "What's your name?"},
        {"type": "image_url", "image_url": {"url": "tos://bucket/1.jpg"}}  
      ]
    },
    {
      "role": "assistant",
      "content": [{"type": "text", "text": "My name is doubao."}],  
      "loss_weight": 1.0
    },
    {
      "role": "user",
      "content": [{"type": "text", "text": "How to learn Python?"}]  
    },
    {
      "role": "assistant",             # 最后一个message的role必须为assistant
      "choices": [                     # 可以有多个response
        {
          "content": [{"type": "text", "text": "I don't know!"}], 
          "score": 0.5,                # 该response的得分 
          "reasoning_content": "xxx",
          "lm_loss_mask": 0            # 这个response是否训练lm_head，默认为0
        },
        {
          "content": [{"type": "text", "text": "Check python doc yourself"}],  
          "score": 0.1,
          "lm_loss_mask": 0
        },
        {
          "content": [{"type": "text", "text": "It's so easy. First, you need to learn Python syntax..."}],  
          "score": 1.0,
          "lm_loss_mask": 0
        },
        {
          "content": [{"type": "text", "text": "It's so easy. First, you need to learn Python syntax..."}],  
          "score": 1.0,
          "lm_loss_mask": 0
        }
      ]
    }
  ],
  "thinking": "disabled"  
}
```

每行一条JSON格式的数据：

* **messages** *(list, required)*: 描述一个对话列表
   * **role** *(str, required)*: 角色，system、user、assistant、tool中的一个。**最后一个message的role必须是assistant，且仅最后一轮对话可以携带多个回复的偏序对**
   * **content** *(list|str, required): 非最后一轮的对话内容。其中 user 的 message 可通过数组混排图文。*
   * **choices** *(list, required)*: 对话内容文本或者一个list对象，仅可以在最后一轮对话中填写，且list长度在2～5之间。list的元素为dict，包含以下字段：
      * **content** *(list|str, required)*：文本内容
      * **reasoning_content** *(str, optional)*: 深度思考内容，只允许最后一个 role 为 assistant 的 message 携带。
         * 如果打开 lm_loss_mask，则需要注意将非最后一轮的 assistant 对话 loss_weight 置为 0，否则将影响模型的深度思考能力
      * **score** *(float, required)*：偏好值，范围在0～1的浮点数，越高表示越偏好
      * **lm_loss_mask** *(optional, required)*：该条消息是否计算 sft loss，默认为 0，即不计算 sft loss
   * **loss_weight** *(float, optional)*: 该条 message loss 的加权系数，默认为 1
* **thinking** (str, 支持该参数的模型必填、不支持的禁填)：部分具备深度思考能力的模型支持通过 **thinking** 字段控制是否启用深度思考能力。
   * 不同模型可配置的枚举值不同，更多模型请参考[ ](https://www.volcengine.com/docs/82379/1330310#%E6%B7%B1%E5%BA%A6%E6%80%9D%E8%80%83%E8%83%BD%E5%8A%9B)[深度思考能力](/docs/82379/1330310#898d064d)。
         
- 模型版本 | enabled | disabled | auto
- doubao-seed-1-6-250615 | ✅ | ✅ | ✅
- doubao-seed-1-6-flash-250615 | ✅ | ✅ | ❌
- doubao-1-5-vision-pro-250328 | 不支持该参数，不可携带
- doubao-1-5-pro-32k-250115 | 不支持该参数，不可携带
- doubao-1-5-lite-32k-250115 | 不支持该参数，不可携带

   * `enabled` 表示强制模型进行思考，此时choices中的assistant返回要携带 reasoning_content 字段，否则将影响模型效果
   * `disabled` 表示强制模型不进行思考，此时choices中的assistant返回不能携带 reasoning_content 字段，否则将影响模型效果
   * `auto` 表示允许模型根据问题自主判断是否需要思考，简单题目直接回答。此模式下用户自主决定是否希望模型思考。如果希望模型思考，则在此时choices中的content携带 reasoning_content 字段，否则则不携带。
   * 对于上述格式要求，存量数据集可参考[附1：数据集Thinking字段处理工具](/docs/82379/1099461#089b8570)进行格式转换。

# 强化学习（GRPO/PPO）
## 文本生成模型
### **格式示例**
样本格式：为JSONL文件（ **JSON Lines**，是一种轻量级的文本文件格式，核心规则 **每一行对应一个独立的、合法的 JSON 对象**），需确保**单个对话样本独占一行**，示例如下。
```Plain Text
{"messages":[{"role":"system","content":"你是一个擅长数学计算的人工智能助手。"},{"role":"user","content":"1+1=？"}],"extra":{"answer":"2"},"thinking":"disabled"}
```

### 格式说明
为便于展示各个字段关系，将JSONL格式文件的一条数据展开，如下：
```JSON
{
  "messages": [
    {
      "role": "system",
      "content": "你是一个擅长数学计算的人工智能助手。"
    },
    {
      "role": "user",
      "content": "1+1=？"
    }
  ],
  "extra": {
    "answer": "2"
  },
  "thinking": "disabled"
}
```

每一条JSON格式的数据：

* **messages** *(list, required)*: 描述一个对话列表
   * **role** *(str, required)*: 角色，system、user、assistant、tool中的一个。**最后一个message的role必须是user或tool**
   * **content** *(str, required)*: 文本的对话内容。
* **extra** (dict, optional): 用于携带 reward function与custom rollout所需的字段
* **thinking** (str, 支持该参数的模型必填、不支持的禁填)：部分具备深度思考能力的模型支持通过 **thinking** 字段控制是否启用深度思考能力。
   * 不同模型可配置的枚举值不同，更多模型请参考[深度思考能力](/docs/82379/1330310#898d064d)。
   
- 模型版本 | enabled | disabled | auto
- doubao-seed-1-6-250615 | ✅ | ✅ | ✅
- doubao-seed-1-6-flash-250615 | ✅ | ✅ | ❌
- doubao-1-5-vision-pro-250328 | 不支持该参数，不可携带
- doubao-1-5-pro-32k-250115 | 不支持该参数，不可携带
- doubao-1-5-lite-32k-250115 | 不支持该参数，不可携带

   * `enabled` 表示强制模型进行思考，此时模型rollout过程预期会输出reasoning_content。
   * `disabled` 表示强制模型不进行思考, 此时模型rollout过程预期不会输出reasoning_content。
   * `auto` 表示允许模型根据问题自主判断是否需要思考，复杂问题进行思考，简单问题直接回答。
   * 对于上述格式要求，存量数据集可参考[附1：数据集Thinking字段处理工具](/docs/82379/1099461#089b8570)进行格式转换。

## 多模态模型（视觉理解）
整体对齐文本生成模型，其中user message的content可以为数组用于混排图文。具体格式参照[图片传入方式](/docs/82379/1099461#6609c8a3)
# 继续预训练
## 文本生成模型
### **格式示例**
样本格式：为JSONL文件（ **JSON Lines**，是一种轻量级的文本文件格式，核心规则 **每一行对应一个独立的、合法的 JSON 对象**），需确保**单个对话样本独占一行**，示例如下。您也可以[下载样例文件](https://ark-cdn.tos-cn-beijing.volces.com/samples/ContinuePretraining_Text_Sample.jsonl)阅读。
```Plain Text
{"text": "火山方舟通过稳定可靠的安全互信方案，保障模型提供方的模型安全与模型使用者的信息安全，加速大模型能力渗透到千行百业，助力模型提供方和使用者实现商业新增长。"}
{"text": "支持运行超大规模的分布式任务，包含多种预置算法框架和自定义算法框架。提供稳定、灵活、高性能的机器学习训练环境。"}
{"text": "支持多种框架的模型在异构硬件上的一键部署，具有高吞吐、低延时、实时扩缩容等特点，使推理服务更具弹性和容错性。"}
```

### 格式说明
为便于展示各个字段关系，将 JSONL 格式文件的一条数据展开，如下：
```JSON
{
    "text": "火山引擎机器学习平台是面向机器学习应用开发者，提供【开发机】和【自定义训练】等丰富建模工具、多框架高性能模型推理服务的企业级开发平台，支持从数据托管、代码开发、模型训练、模型部署的全生命周期工作流。"
}
{
    "text": "支持运行超大规模的分布式任务，包含多种预置算法框架和自定义算法框架。提供稳定、灵活、高性能的机器学习训练环境。"
}
{
    "text": "支持多种框架的模型在异构硬件上的一键部署，具有高吞吐、低延时、实时扩缩容等特点，使推理服务更具弹性和容错性。"
}
```

每行一条JSON格式的数据：

* **text** *(str, required)*: 想要训练的字符串文本。每条样本不限制text长度，如果超长将自动根据模型最大能支持的token拆成多个样本，因此样本总数可能会超过数据的行数

# 附1：数据集Thinking字段处理工具
您可以通过脚本将不含`thinking`的数据集转换为含`thinking`的数据集：

* 对于数据集中不含`reasoning_content`的样本，将`thinking`赋值为`disabled`。
* 含`reasoning_content`的样本，则赋值为`enabled`。

您可下载 [此脚本](https://ark-project.tos-cn-beijing.volces.com/jsonl/dataset_converter.zip) 对数据集进行转换，使您的数据集符合精调模型数据格式规范。
# 附2：多轮reasoning_content的样本文件拆分
您可参照以下脚本拆分样本:

* 代码实现：输入一个JSONL样例文件，输出处理好的训练数据到指定文件夹。
* 使用示例：python main.py --input test.jsonl --output res-folder

```Python
import json
import os
import argparse
from process import process_sample
from typing import List, Dict, Any

def process_sample(sample: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    处理单个样例数据，根据规则生成一组处理后的样例
    
    参数:
        sample: 包含消息的字典，格式如案例所示
        
    返回:
        处理后的样例列表
    """
    messages = sample["messages"]
    
    # 用于存储处理后的消息
    message_result_list = []
    processed_messages = []
    not_set_loss_weight_index = []
    
    for i in range(0, len(messages)):
        current = messages[i]
        
        # 最后一个消息不需要处额外理
        if i == len(messages) - 1:
            processed_messages.append(current)
            message_result_list.append(processed_messages.copy())
            break
            
        # 只处理role为assistant的消息
        if current.get("role") == "assistant":
            # 检查是否存在loss_weight: 0
            has_loss_weight_zero = "loss_weight" in current and current["loss_weight"] == 0
            # 检查是否存在reasoning_content
            has_reasoning_content = "reasoning_content" in current
            
            # 情况1: 不存在loss_weight:0, 且存在reasoning_content
            if not has_loss_weight_zero and has_reasoning_content:
                # 拆分出当前消息, 并添加到消息列表中
                previous_messages = [msg.copy() for msg in processed_messages]
                previous_messages.append(current.copy())
                message_result_list.append(previous_messages.copy())
                
                # 处理当前消息, 不保留reasoning_content, 并记录loss_weight:0
                new_assistant_msg = current.copy()
                del new_assistant_msg["reasoning_content"]
                new_assistant_msg["loss_weight"] = 0
                
                # 将处理后的消息段保存下来，用于后续使用
                processed_messages.append(new_assistant_msg)
                current = []
                
                # 处理之前未设置loss_weight的消息，确保每条消息只被用于一次训练
                for idx in not_set_loss_weight_index:
                    processed_messages[idx]["loss_weight"] = 0
                not_set_loss_weight_index = []
                
            # 情况2: 存在loss_weight:0
            elif has_loss_weight_zero:
                # 不做拆分，不保留reasoning_content
                new_assistant_msg = current.copy()
                if "reasoning_content" in new_assistant_msg:
                    del new_assistant_msg["reasoning_content"]
                
                processed_messages.append(new_assistant_msg)
            # 情况3: 不存在reasoning_content
            else:
                processed_messages.append(current)
                # 记录下当前未设置loss_weight的消息
                not_set_loss_weight_index.append(i)
        else:
            # 非assistant角色的消息直接添加
            processed_messages.append(current)
    
    # 生成最终的样例列表
    processed_samples = []
    for messages in message_result_list:
        processed_sample = sample.copy()
        processed_sample["messages"] = messages
        processed_samples.append(processed_sample)
    
    return processed_samples

def main():
    parser = argparse.ArgumentParser(description='处理JSONL样例数据并输出到文件夹')
    parser.add_argument('--input', required=True, help='输入的JSONL文件路径')
    parser.add_argument('--output', required=True, help='输出文件夹路径')  
    
    args = parser.parse_args()
    
    # 读取JSONL文件
    samples = []
    try:
        with open(args.input, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():  # 跳过空行
                    samples.append(json.loads(line))
    except FileNotFoundError:
        print(f"错误: 找不到输入文件 {args.input}")
        return
    
    # 处理所有样例
    all_processed_samples = []
    for sample in samples:
        processed_samples = process_sample(sample)
        all_processed_samples.extend(processed_samples)
    
    # 写入JSONL文件到文件夹
    os.makedirs(args.output, exist_ok=True)
    
    try:
        # 确保输出目录存在
        os.makedirs(args.output, exist_ok=True)
        
        # 构建完整输出路径
        output_path = os.path.join(args.output, f"result.jsonl")
        
        # 将所有样本写入JSONL文件
        with open(output_path, 'w', encoding='utf-8') as f:
            for sample in all_processed_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        
        print(f"处理完成，已生成JSONL文件，包含 {len(all_processed_samples)} 个样本，保存在 {output_path}")
    except Exception as e:
        print(f"错误: 写入文件失败 - {e}")

if __name__ == "__main__":
    main()
```

# 附3：对视频内容进行抽帧处理
当您需要对视频内容进行理解时，可以对视频进行抽帧，生成若干图片后，再将图片结合文本作为提示词内容，请求视频理解的模型服务。下面是使用 Python 脚本实现的方案，供您参考。
## 代码流程

1. 视频文件读取：从本地路径读取视频文件。
2. 对视频进行抽帧，可以支持 2 种策略：
   1. `CONSTANT_INTERVAL`：固定时间间隔抽帧，如 1s 一帧，时间间隔可以调整。
   2. `EVEN_INTERVAL`：固定总帧数均匀抽帧，如不管视频时长，总共抽 20 帧，抽取的总帧数可以调整。
3. 压缩帧图像：为了降低网络传输时延，以及减少 token 用量，将图像等比例压缩至 640*480 以内。
4. 请求视觉理解模型：组装请求体，包含提示词（Prompt）及所有帧图像的 Base64，并使用方舟`ChatCompletion` 接口调用模型服务。

## 示例代码
### 环境配置
在运行脚本前，您需要配置好 API Key、获取Model ID，具体方法请参见：
* [获取 API Key](https://console.volcengine.com/ark/region:ark+cn-beijing/apiKey) 
* [开通模型服务](https://console.volcengine.com/ark/openManagement)
* 在 [模型列表](/docs/82379/1330310) 获取所需 Model ID 
   * 通过 Endpoint ID 调用模型服务，请参考 [获取 Endpoint ID（创建自定义推理接入点）](/docs/82379/1099522)。

> 创建推理接入点时，请选择支持视觉理解的大模型，具体可参见[视觉理解能力](/docs/82379/1330310#ff5ef604)。

运行脚本前，请先在终端中安装依赖包。
```Shell
pip install opencv-contrib-python
pip install 'volcengine-python-sdk[ark]'
```

### 实现脚本以及配置
配置您的变量，包括：

* `<VIDEO_FILE_PATH>`替换您的视频文件的路径。
* `<MODEL>`替换为模型的Model ID。
* `prompt = "描述视频内容"`配置您的文本提示，这里仅做示例使用。
* 选择您的抽帧策略。

```Python
...
# 1. CONSTANT_INTERVAL：固定时间间隔抽帧，可以调整时间间隔
# 2. EVEN_INTERVAL：固定总帧数均匀抽帧，可以调整总帧数
        extraction_strategy=Strategy.CONSTANT_INTERVAL,
        # extraction_strategy=Strategy.EVEN_INTERVAL,
...
```

### 完整的 Python 脚本
```Python
from typing import Optional
# Import the OpenCV library for video processing
import cv2
# Import the time library for timing
import time
# Import the file operation library for directory deletion
import shutil
# Import the enum library for defining frame extraction strategies
from enum import Enum

# Import the Base64 encoding library for converting images to Base64 strings
import base64
# Import the operating system library for file path operations
import os
# Install SDK:pip install 'volcengine-python-sdk[ark]'
from volcenginesdkarkruntime._exceptions import ArkAPIError

# Define the frame extraction strategy enumeration class
class Strategy(Enum):
    # Fixed interval frame extraction strategy, e.g., extract one frame every 1 second
    CONSTANT_INTERVAL = "constant_interval"
    # Even interval frame extraction strategy, extract frames evenly from the entire video based on the set maximum number of frames
    EVEN_INTERVAL = "even_interval"

def preprocess_video(
        video_file_path: str,
        output_dir: str,
        extraction_strategy: Optional[Strategy] = Strategy.EVEN_INTERVAL,
        interval_in_seconds: Optional[float] = 1,
        max_frames: Optional[int] = 10,
        keyframe_naming_template: str = "frame_{:04d}.jpg",
) -> list[str]:
    """Extract frames from video according to the specified strategy
    Args:
        video_file_path (str): Path to the video file
        output_dir (str): Output directory
        extraction_strategy (Optional[Strategy], optional): Frame extraction strategy.
             Constant interval (e.g., extract one frame every 1 second) or
             Even interval (extract frames evenly from the entire video based on the set maximum number of frames)
             Defaults to constant interval (1 frame per second).
        interval_in_seconds (Optional[float], optional): Time interval for constant interval frame extraction. Defaults to 1 second per frame.
        max_frames (Optional[int], optional): Maximum number of frames to extract. Defaults to 10 frames.
        keyframe_naming_template (_type_, optional): Naming template for extracted key frame images
    Returns:
        list[str]: List of paths to the extracted key frame images
    """
    # Check if the output directory exists; create it if it does not
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    # Open the video file using OpenCV
    cap = cv2.VideoCapture(video_file_path)
    # Get the frame rate of the video
    fps = cap.get(cv2.CAP_PROP_FPS)
    # Get the total number of frames in the video
    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # Select the frame interval based on the strategy
    if extraction_strategy == Strategy.CONSTANT_INTERVAL:
        # Calculate the frame interval for constant interval extraction
        frame_interval = int(fps * interval_in_seconds)
    elif extraction_strategy == Strategy.EVEN_INTERVAL:
        # Calculate the frame interval for even interval extraction
        frame_interval = int(length / max_frames)
    else:
        # Throw an exception if the strategy is invalid
        raise ValueError("Invalid extraction strategy")
    # Initialize the frame counter
    frame_count = 0
    # Initialize the list of key frames
    keyframes = []
    # Loop to read video frames
    while True:
        # Read one frame
        ret, frame = cap.read()
        # Exit the loop if frame reading fails
        if not ret:
            break
        # Check if the current frame is a key frame
        if frame_count % frame_interval == 0:
            # Generate the filename for the key frame
            image_path = os.path.join(
                output_dir, keyframe_naming_template.format(len(keyframes))
            )
            # Save the key frame as an image
            cv2.imwrite(
                image_path,
                frame,
            )
            # Add the key frame path to the list
            keyframes.append(image_path)
        # Increment the frame counter
        frame_count += 1
        # Exit the loop if the maximum number of key frames is reached
        if len(keyframes) >= max_frames:
            break

    print("[Video Frame Extraction Completed]")
    print("[Number of Extracted Frames]", len(keyframes))
    # Return the list of key frame paths
    return keyframes

def resize(image):
    """
    Resize the image to fit the specified dimensions.
    Args:
        image (numpy.ndarray): Input image in numpy array format.
    Returns:
        numpy.ndarray: Resized image.
    """
    # Get the original height and width of the image
    height, width = image.shape[:2]
    # Determine the target dimensions based on the image's aspect ratio
    if height < width:
        target_height, target_width = 480, 640
    else:
        target_height, target_width = 640, 480
    # Return the original image if its dimensions are already less than or equal to the target dimensions
    if height <= target_height and width <= target_width:
        return image
    # Calculate the new height and width while maintaining the image's aspect ratio
    if height / target_height < width / target_width:
        new_width = target_width
        new_height = int(height * (new_width / width))
    else:
        new_height = target_height
        new_width = int(width * (new_height / height))
    # Resize the image
    return cv2.resize(image, (new_width, new_height))

# Define a method to resize the image at the specified path to an appropriate size and convert it to Base64 encoding
def encode_image(image_path: str) -> str:
    """
    Encode the image at the specified path
    Args:
        image_path (str): Path to the image file
    Returns:
        str: Encoded image string
    """
    # Read the image
    image = cv2.imread(image_path)
    # Resize the image
    image_resized = resize(image)
    # Encode the image in JPEG format
    _, encoded_image = cv2.imencode(".jpg", image_resized)
    # Convert the encoded image to a Base64 string
    return base64.b64encode(encoded_image).decode("utf-8")

def construct_messages(image_paths: list[str], prompt: str) -> list[dict]:
    """
    Construct a list of messages containing text and images.
    Args:
        image_paths (list[str]): List of image file paths.
        prompt (str): Text prompt.
    Returns:
        list[dict]: List of messages containing text and images.
    """
    print("[Assembling Request Parameters]")
    # Initialize the content list with a text message
    content = [
        {
            "type": "text",
            "text": prompt,
        },
    ]
    # Iterate through the list of image paths
    for image_path in image_paths:
        # Construct an image URL message for each image path
        content.append(
            {
                "type": "image_url",
                "image_url": {
                    # Convert the image to a data URL using Base64 encoding
                    "url": f"data:image/jpeg;base64,{encode_image(image_path)}",
                    # Specify the image detail level as low
                    "detail":"low"
                },
            }
        )
    # Return the list of messages containing text and images
    return [
        {
            "role": "user",
            "content": content,
        }
    ]

if __name__ == "__main__":
    # Replace with the path to your video file
    video_path = "<VIDEO_FILE_PATH>"
    # API Key retrieved from environment variables; configure your API Key in advance
    ark_api_key = os.environ.get("ARK_API_KEY")
    # Replace <MODEL> with the Model ID of your model
    model_id = "<MODEL>"
    # Replace with your text prompt; this is for example use only
    prompt = "Describe the video content"
    since = time.time()
    # Delete historical frames
    if os.path.exists("videoFrames"):
        shutil.rmtree("videoFrames")
        
    selected_images = preprocess_video(
        video_file_path=video_path,
        output_dir="videoFrames",
        
        # Two frame extraction strategies
        # 1. CONSTANT_INTERVAL: Extract frames at a fixed time interval; adjust the interval as needed
        # 2. EVEN_INTERVAL: Extract a fixed total number of frames evenly; adjust the total number as needed
        extraction_strategy=Strategy.CONSTANT_INTERVAL,
        # extraction_strategy=Strategy.EVEN_INTERVAL,
        
        # Frame interval for constant interval extraction strategy
        interval_in_seconds=1,

        # Maximum number of frames to extract from the video
        max_frames=20
    )

    print("[Frame Image Compression Completed]")
    preprocess_time_cost = time.time() - since

    client = Ark(api_key=ark_api_key)
    since = time.time()
    try:
        response = client.chat.completions.create(
            model=model_id,
            messages=construct_messages(
                selected_images,
                prompt,
            )
        )

        print("[Video Understanding Result]\n", response.choices[0].message.content)
        print("\n[Total Tokens Consumed]", response.usage.total_tokens)
        print("- Input Tokens:", response.usage.prompt_tokens)
        print("- Output Tokens:", response.usage.completion_tokens)

    except ArkAPIError as e:
        print(e)

    api_time_cost = time.time() - since
    print("\n[End-to-End Total Time Consumption]", round(preprocess_time_cost + api_time_cost, 2))
    print("- Video Preprocessing Time:", round(preprocess_time_cost, 2))
    print("- Result Generation Time:", round(api_time_cost, 2))
```
